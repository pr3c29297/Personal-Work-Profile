{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3(B): Building a Robot Cleaner with Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acknowledgment\n",
    "\n",
    "You are required to acknowledge the following statement by entering your full name, SID, and date below:\n",
    "\n",
    "\"By continuing to work on or submit this deliverable, I acknowledge that my submission is entirely my independent original work done exclusively for this assessment item. I agree to: \n",
    "\n",
    "* Submit only my independent original work\n",
    "* Not share answers and content of this assessment with others\n",
    "* Report suspected violations to the instructor\n",
    "\n",
    "Furthermore, I acknowledge that I have not engaged and will not engage in any activities that dishonestly improve my results or dishonestly improve/hurt the results of others, and that I abide to all academic honor codes set by the University.\"\n",
    "\n",
    "**Your full name:**  \n",
    "Pang Fong Chun  \n",
    "**Your SID:**  \n",
    "3035281299  \n",
    "**Date:**  \n",
    "15 Jul 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction <a id='section_1'></a>\n",
    "In this part of the assignment, you will implement the Reinforcement Learning (RL) algorithms, and use the models learned by these algorithms to make decisions on cleaning robot navigation problem. You are required to complete the lines between __START YOUR CODE HERE__ and __END YOUR CODE HERE__ (if applicable) and to execute each cell. Within each coding block, you are required to enter your code to replace `None` after the `=` sign (except otherwise stated). You are not allowed to use other libraries or files than those provided in this assignment. When entering your code, you should not change the names of variables, constants, and functions already listed. \n",
    "\n",
    "**Contents**\n",
    "\n",
    "* [1. Introduction](#section_1)\n",
    "* [2. Learning Environment](#section_2)\n",
    "* [3. Q-Agent](#section_3)\n",
    "* [4. Agent Environment Interaction](#section_4)\n",
    "* [5. Cleaning Performance Evaluation](#section_5)\n",
    "    * [5.1. Empty Room with One Cell to Clean](#section_51)\n",
    "    * [5.2. Empty Room with Two Cells to Clean](#section_52)\n",
    "    * [5.3. Room with Obstacles and Two Cells to Clean](#section_53)\n",
    "* [6. Marking Scheme and Submission](#section_6)\n",
    "* [7. Summary](#section_7)\n",
    "\n",
    "Before we begin with the exercises, we need to import all libraries required for this programming exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scientific and vector computation for python\n",
    "import numpy as np\n",
    "\n",
    "# Plotting library\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "# library for data copy\n",
    "from copy import deepcopy\n",
    "\n",
    "# package for display\n",
    "from IPython import display\n",
    "\n",
    "# the Room environment\n",
    "import Room\n",
    "from Room import *\n",
    "\n",
    "# tells matplotlib to embed plots within the notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Learning Environment <a id='section_2'></a>\n",
    "You will use a pre-defined environment (a room) to train a robot (or agent) to learn to clean a room. The robot has a set of sensors to observe the state of its environment, and a set of actions it can perform to change the state. You will implement a reinforcement learning (RL) algorithm to enable the robot to learn. The environment (the class named \"Room()\") has been set up for you as follows.\n",
    "\n",
    "![Room Environment](./room.png \"Room\")\n",
    "\n",
    "1. **Grid**: the room is split into 49 (=7x7) cells. The position of the robot cleaner must be in one of these cells.\n",
    "2. **Goal**: the goal of the robot (green circle) is to clean all the cell(s) labeled with **red stars** by minimizing the energy to be used to navigate to the cell(s) (assuming unlimited battery capacity). \n",
    "3. **Obstacle**: each obstacle is labeled by a **black cell** that the robot should avoid colliding with.\n",
    "\n",
    "All agents will learn by interacting with the environment. You can create and initialize such the environment (room) with the following statement:\n",
    "```\n",
    "    env = Room(size=(5,5), goal_num=3, obstacle_num=2)\n",
    "```\n",
    "where the input `size=` denotes the configuration of the room; \"goal_num\" is the number of cells (labeled with stars) that the robot should clean; \"obstacle_num\" is the number of obstacles. After you initialized the environment named by `env`, you can use three types of functions:\n",
    "\n",
    "1. `env.reset()`: this function is used to reset the room environment. The positions of the robot and goals will be re-assigned randomly. The output of this function is the `state` vector (i.e., states).\n",
    "2. `env.step()`: is used to let the agent interact with the environment. The agent enters `action` to the function that returns the `state` and `reward`. The boolean parameter `done` indicates whether the learning episode is ending (if it happens, then the function \"env.reset()\" must be used to reset the environment).\n",
    "3. `env.render()`: is used to visualize the current situation of environment (grid world), goal cells (red star) and your cleaning robot (green circle).\n",
    "\n",
    "##### Reward Computation\n",
    "1. energy usage: a penalty $r_{energy} = -0.01$ will be incurred each time the `env.step()` is used.\n",
    "2. reaching a goal: when the robot reaches a goal, a reward $r_{goal} = +1$ will be added and the color of goal star will turn from red to blue (no more reward will be given when it is re-visited). If all goals have been reached, then the learning episode will end.\n",
    "3. boundary penalty: any action that will lead the robot to go out of the room boundary will incur a penalty $r_{boundary} = -0.01$, and the robot will stay in the cell prior to taking this action.\n",
    "4. obstacle penalty: when the robot collide with an obstacle, a penalty $r_{obstacle} = -1$ will be incurred. Then, this episode will end. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Test Block 1]:** Test code for class `Room()`.\n",
    "After defining the environment, please run the following demo `room_0 = Room()`environment to see how the robot interacts with the environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random robot cleaning\n",
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = fig.subplots(1,1)\n",
    "room_0 = Room(goal_num=2, obstacle_num=3)\n",
    "state = room_0.reset()\n",
    "\n",
    "for _ in range(100):\n",
    "    action = np.random.randint(room_0.action_space)\n",
    "    state, reward, done = room_0.step(action)\n",
    "    room_0.render()\n",
    "    plt.pause(0.01)\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        state = room_0.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Q-Agent <a id='section_3'></a>\n",
    "One of the most important tasks to use reinforcement learning is to define a learning agent. In this assignment, your task is to implement the Q-learning algorithm. The update rule of Q-learning algorithm is defined as follows:\n",
    "\n",
    "$$ \\hat{Q}(s, a) \\leftarrow r + \\gamma\\max_{a^{\\prime}}\\hat{Q}(s^{\\prime}, a^{\\prime}) \\ . \\tag{1}$$ \n",
    "\n",
    "**Task 1:** <a id='task_1'></a> in detail, you should:\n",
    "\n",
    "1. assign the input `gamma` to attribute `self.gamma` (1 line)\n",
    "2. assign the input `learning_rate` to attribute `self.learning_rate` (1 line)\n",
    "3. assign the input `epsilon` to attribute `self.epsilon` (1 line)\n",
    "\n",
    "**Task 2:** <a id='task_2'></a> In this task, you will implement the [$\\epsilon$-greedy](https://stanford-cs221.github.io/autumn2020-extra/modules/mdps/epsilon-greedy.pdf) rule for the exploration and exploitation in an unknown environment ([this article provides a practical explanation of the algorithm](https://moodle.hku.hk/mod/resource/view.php?id=2589880)).\n",
    "\n",
    "1. generate a random number in uniform distribution between 0 and 1. You can use the function `np.random.uniform()` to generate this number and compare this number with `self.epsilon`. (1 line)\n",
    "2. if the number is larger than the `self.epsilon`, get current action value based on the current state of Q table. (3 lines)\n",
    "    * Please first transform the list data type to tuple with the function `tuple()` with the same name. Save it in `tuple_state`\n",
    "    * use `self.q_table[]` and `tuple_state` to get the q value vector of current state, save it in `q_value`\n",
    "    * extract the element index of the maximal value in the `q_value`, save it in `action`\n",
    "3. if the number is smaller than `self.epsilon`, please select one action from [0, 1, 2, 3] randomly (note: `self.action_n` = 4, i.e., four possible actions). You can use the `np.random.randint()` to finish this function. (1 line)\n",
    "\n",
    "**Task 3:** <a id='task_3'></a> In this task, you will:\n",
    "\n",
    "1. compute `td_target` by applying the updating rule of Q-learning (Equation (1)) to get next state's estimated reward (note: you can use `.max()` to find the maximum scalar value within a numpy array; you need to multiply `(1 - done)` with the $\\max$ function in Equation (1)) (1 line)\n",
    "2. assign `state` to `q_state` and append `action` to `q_state` (2 lines)\n",
    "3. compute `td_error` by subtracting the q value vector of current state from `td_target` (1 line)\n",
    "4. update the Q-table of current state by adding the product of learning rate and `td_error` (1 line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QAgent:\n",
    "    def __init__(self, env, gamma=0.9, learning_rate=0.1, epsilon=0.2):\n",
    "        '''\n",
    "            - env: environment for learning\n",
    "            - gamma: reward discount factor\n",
    "            - learning_rate: learning rate\n",
    "            - epsilon: hyperparameter for exploration\n",
    "        '''\n",
    "        # task 1:\n",
    "        # ====================== START YOUR CODE HERE ======================\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epsilon = epsilon\n",
    "        # ====================== END YOUR CODE HERE ========================\n",
    "        self.action_n = env.action_space\n",
    "        # q-table\n",
    "        q_table_dimension = env.state_space\n",
    "        q_table_dimension.append(env.action_space)\n",
    "        self.q_table = np.random.normal(0.0, 0.0001, q_table_dimension)\n",
    "        \n",
    "    def decide(self, state):\n",
    "        # task 2:\n",
    "        # ====================== START YOUR CODE HERE ======================\n",
    "        if np.random.uniform() > self.epsilon:\n",
    "            tuple_state = tuple(state)\n",
    "            q_value = self.q_table[tuple_state]\n",
    "            action = np.argmax(q_value)\n",
    "        else:\n",
    "            action = np.random.randint(4)\n",
    "        # ====================== END YOUR CODE HERE ========================\n",
    "        return action\n",
    "    \n",
    "    def learn(self, state, action, reward, next_state, done):\n",
    "        # task 3:\n",
    "        # ====================== START YOUR CODE HERE ======================\n",
    "        td_target = reward+self.gamma*max(self.q_table[next_state])*(1-done)\n",
    "        q_state = state\n",
    "        q_state.append(action)\n",
    "        td_error = td_target-self.q_table[state]\n",
    "        self.q_table[state] += learning_rate*td_error\n",
    "        # ====================== END YOUR CODE HERE ========================"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Agent-Environment Interaction <a id='section_4'></a>\n",
    "In this part, you will define the function for the interaction between agent and environment. In detail, you will:\n",
    "\n",
    "**Task 4:** <a id='task_4'></a>\n",
    "\n",
    "1. get the current action from the agent with its function `decide()`. Save the resulting action in `action` (1 line)\n",
    "2. use the `action` as input to the function `env.step()` to generate `next_state`, `reward`, and `done` (check if the current state is a terminal state of this episode) (1 line)\n",
    "3. store the obtained `reward` in `episode_reward` (1 line)\n",
    "\n",
    "**Task 5:** <a id='task_5'></a> \n",
    "\n",
    "1. when `train` is true, please use your implemented `agent.learn()` function to update the Q-table in the agent with existing variables as inputs (1 line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agent_env_interaction(env, agent, max_iter=50, train=False):\n",
    "    episode_reward = 0\n",
    "    state = env.reset()\n",
    "    for _ in range(max_iter):\n",
    "        # task 4:\n",
    "        # ====================== START YOUR CODE HERE ======================\n",
    "        action = agent.decide(state)\n",
    "        next_state, reward, done = None, None, None\n",
    "        episode_reward += reward\n",
    "        # ====================== END YOUR CODE HERE ========================\n",
    "        if train:\n",
    "            # task 5:\n",
    "            # ====================== START YOUR CODE HERE ======================\n",
    "            agent.learn(state, action, reward, next_state, done)\n",
    "            # ====================== END YOUR CODE HERE ========================\n",
    "        if done:\n",
    "            break\n",
    "        state = next_state\n",
    "    return episode_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Cleaning Performance Evaluation <a id='section_5'></a>\n",
    "In this section, you will train multiple agents in different environments. Note that the agents cannot guarantee 100% successful completion of the tasks. You can re-run the evaluation to watch the overall performance of the agent. \n",
    "### 5.1. Empty Room with One Cell to Clean<a id='section_51'></a>\n",
    "#### 5.1.1. Training Environment Setting <a id='section_511'></a>\n",
    "The learning task is to let the robot clean one target cell in an empty room (without obstacle). The environment will be configured to have one goal and no obstacle. In detail, you will:\n",
    "\n",
    "**Task 6:** <a id='task_6'></a> \n",
    "##### Task 6(A)\n",
    "1. create a `Room()` environment and save it in `room_1`. Please set the input `goal_num=` and `obstacle_num=` correctly. (1 line)\n",
    "2. create a `QAgent()` robot and save it in `robot_1`. Please set the input `epsilon=` as 0.2 (you can also set another value later for comparison). (1 line)\n",
    "##### Task 6(B)\n",
    "3. get the reward of one episode with your implemented function `agent_env_interaction()`. Do not forget to set the input `train=` to be `True`. Then, save the result in `episode_reward`. (1 line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'decide' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(episode))\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# task 6(B):\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# ====================== START YOUR CODE HERE ======================\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m episode_reward \u001b[38;5;241m=\u001b[39m \u001b[43magent_env_interaction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mroom_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrobot_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# ====================== END YOUR CODE HERE ========================\u001b[39;00m\n\u001b[1;32m     17\u001b[0m episode_rewards\u001b[38;5;241m.\u001b[39mappend(episode_reward)\n",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36magent_env_interaction\u001b[0;34m(env, agent, max_iter, train)\u001b[0m\n\u001b[1;32m      3\u001b[0m state \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mreset()\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_iter):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;66;03m# task 4:\u001b[39;00m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# ====================== START YOUR CODE HERE ======================\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43mdecide\u001b[49m(state)\n\u001b[1;32m      8\u001b[0m     next_state, reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     episode_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "\u001b[0;31mNameError\u001b[0m: name 'decide' is not defined"
     ]
    }
   ],
   "source": [
    "episodes = 10000\n",
    "# create environment\n",
    "# task 6(A):\n",
    "# ====================== START YOUR CODE HERE ======================\n",
    "room_1 = Room(goal_num=1, obstacle_num=0)\n",
    "robot_1 = QAgent(room_1, epsilon=0.2)\n",
    "# ====================== END YOUR CODE HERE ========================\n",
    "# training\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    if episode % 1000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "    # task 6(B):\n",
    "    # ====================== START YOUR CODE HERE ======================\n",
    "    episode_reward = agent_env_interaction(room_1, robot_1, train=True)\n",
    "    # ====================== END YOUR CODE HERE ========================\n",
    "    episode_rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.2. Training Performance Visualization <a id='section_512'></a>\n",
    "After training 10000 episodes, the average rewards of each episode are visualized by the following figure. Note that we use a moving average (window size=20) on the recorded reward information. Decreasing the size of the window would show a more drastic fluctuation of rewards during the learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute moving average\n",
    "window_size = 20\n",
    "moving_averaged_rewards = list()\n",
    "for idx in range(len(episode_rewards)):\n",
    "    if idx+window_size < len(episode_rewards):\n",
    "        average_reward = np.mean(episode_rewards[idx:idx+window_size])\n",
    "        moving_averaged_rewards.append(average_reward)\n",
    "\n",
    "# test agent without exploration\n",
    "robot_1.epsilon = 0.\n",
    "test_episode_rewards = [agent_env_interaction(room_1, robot_1, train=False) for _ in range(500)]\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.subplots(1,1)\n",
    "ax.plot(moving_averaged_rewards, color='b', linewidth=1.5)\n",
    "ax.grid(linestyle='--', linewidth=1.5)\n",
    "ax.set_xlabel('Number of episodes')\n",
    "ax.set_ylabel('Episodic rewards')\n",
    "ax.set_title('Average episodic reward = {} / {} = {}'.format(sum(test_episode_rewards), \n",
    "                                                              len(test_episode_rewards), np.mean(test_episode_rewards)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1.3. Robot Behavior Visualization <a id='section_513'></a>\n",
    "You can also visualize the behaviors of your trained cleaning robot in the room by the following code block:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robot clearning with single goal\n",
    "state = room_1.reset()\n",
    "for _ in range(100):\n",
    "    action = robot_1.decide(state)\n",
    "    state, reward, done = room_1.step(action)\n",
    "    room_1.render()\n",
    "    plt.pause(0.01)\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        state = room_1.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Empty Room with Two Cells to Clean <a id='section_52'></a>\n",
    "#### 5.2.1. Training Environment Setting <a id='section_521'></a>\n",
    "The learning task in this section is to let the robot clean two target cells in an empty room (without obstacle). The environment will be configured to have two goals and no obstacle. In detail, you will:\n",
    "\n",
    "**Task 7:** <a id='task_7'></a> \n",
    "##### Task 7(A)\n",
    "1. create a `Room()` environment and save it in `room_2`. Please set the input `goal_num=` and `obstacle_num=` correctly. (1 line)\n",
    "2. create a `QAgent()` robot and save it in `robot_2`. Please set the input `epsilon=` as 0.2 (you can also set another value later for comparison). (1 line)\n",
    "##### Task 7(B)\n",
    "3. get the reward of one episode with your implemented function `agent_env_interaction()`. Do not forget to set the input `train=` to be `True`. Then, save the result in `episode_reward`. (1 line)\n",
    "\n",
    "This part requires aproximately 4-6 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 100000\n",
    "# create environment\n",
    "# task 7(A):\n",
    "# ====================== START YOUR CODE HERE ======================\n",
    "room_2 = None\n",
    "robot_2 = None\n",
    "# ====================== END YOUR CODE HERE ========================\n",
    "# training\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    if episode % 1000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "    # task 7(B):\n",
    "    # ====================== START YOUR CODE HERE ======================\n",
    "    episode_reward = None\n",
    "    # ====================== END YOUR CODE HERE ========================\n",
    "    episode_rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.2. Training Performance Visualization <a id='section_522'></a>\n",
    "After training 100000 episodes, the average rewards of each episode are visualized by the following figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute moving average\n",
    "window_size = 20\n",
    "moving_averaged_rewards = list()\n",
    "for idx in range(len(episode_rewards)):\n",
    "    if idx+window_size < len(episode_rewards):\n",
    "        average_reward = np.mean(episode_rewards[idx:idx+window_size])\n",
    "        moving_averaged_rewards.append(average_reward)\n",
    "\n",
    "robot_2.epsilon = 0. # disable exploration\n",
    "test_episode_rewards = [agent_env_interaction(room_2, robot_2, train=False) for _ in range(500)]\n",
    "\n",
    "fig = plt.figure(figsize=(8,6))\n",
    "ax = fig.subplots(1,1)\n",
    "ax.plot(moving_averaged_rewards, color='b', linewidth=1.5)\n",
    "ax.grid(linestyle='--', linewidth=1.5)\n",
    "ax.set_xlabel('Number of episodes')\n",
    "ax.set_ylabel('Episodic rewards')\n",
    "ax.set_title('Average episodic reward = {} / {} = {}'.format(sum(test_episode_rewards), \n",
    "                                                              len(test_episode_rewards), np.mean(test_episode_rewards)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2.3. Robot Behavior Visualization <a id='section_523'></a>\n",
    "You can also visualize the behaviors of your trained cleaning robot in the room by the following code block:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robot clearning with single goal\n",
    "state = room_2.reset()\n",
    "for _ in range(100):\n",
    "    action = robot_2.decide(state)\n",
    "    state, reward, done = room_2.step(action)\n",
    "    room_2.render()\n",
    "    plt.pause(0.01)\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        state = room_2.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Room with Obstacles and Two Cells to Clean <a id='section_53'></a>\n",
    "#### 5.3.1. Training Environment Setting <a id='section_531'></a>\n",
    "The learning task is to let the robot clean two target cells in a room with two obstacles. The environment will be configured to have two goals and two obstacles. In detail, you will:\n",
    "\n",
    "**Task 8:** <a id='task_8'></a> \n",
    "##### Task 8(A)\n",
    "1. create a `Room()` environment and save it in `room_3`. Please set the input `goal_num=` and `obstacle_num=` correctly. (1 line)\n",
    "2. create a `QAgent()` robot and save it in `robot_3`. Please set the input `epsilon=` as 0.2 (you can also set another value later for comparison). (1 line)\n",
    "##### Task 8(B)\n",
    "3. get the reward of one episode with your implemented function `agent_env_interaction()`. Do not forget to set the input `train=` to be \"True\". Then, save the result in `episode_reward`. (1 line)\n",
    "\n",
    "This part requires aproximately 7-10 minutes depending on your hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 150000\n",
    "# create environment\n",
    "# task 8(A):\n",
    "# ====================== START YOUR CODE HERE ======================\n",
    "room_3 = None\n",
    "robot_3 = None\n",
    "# ====================== END YOUR CODE HERE ========================\n",
    "# training\n",
    "episode_rewards = []\n",
    "for episode in range(episodes):\n",
    "    if episode % 1000 == 0:\n",
    "        print(\"Episode: {}\".format(episode))\n",
    "    # task 8(B):\n",
    "    # ====================== START YOUR CODE HERE ======================\n",
    "    episode_reward = None\n",
    "    # ====================== END YOUR CODE HERE ========================\n",
    "    episode_rewards.append(episode_reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.2. Training Performance Visualization <a id='section_532'></a>\n",
    "After training 150000 episodes, the average rewards of each episode are visualized by the following figure.The size of moving average window is 50 due to large vibration in more complex environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute moving average\n",
    "window_size = 50\n",
    "moving_averaged_rewards = list()\n",
    "for idx in range(len(episode_rewards)):\n",
    "    if idx+window_size < len(episode_rewards):\n",
    "        average_reward = np.mean(episode_rewards[idx:idx+window_size])\n",
    "        moving_averaged_rewards.append(average_reward)\n",
    "\n",
    "robot_3.epsilon = 0. # disable exploration\n",
    "test_episode_rewards = [agent_env_interaction(room_3, robot_3) for _ in range(500)]\n",
    "\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "ax = fig.subplots(1,1)\n",
    "ax.plot(moving_averaged_rewards, color='b', linewidth=2)\n",
    "ax.grid(linestyle='--', linewidth=1.5)\n",
    "ax.set_xlabel('Number of episodes')\n",
    "ax.set_ylabel('Episodic rewards')\n",
    "ax.set_title('Average episodic reward = {} / {} = {}'.format(sum(test_episode_rewards), \n",
    "                                                              len(test_episode_rewards), np.mean(test_episode_rewards)));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.3.3. Robot Behavior Visualization <a id='section_533'></a>\n",
    "You can also visualize the behaviors of your trained cleaning robot in the room by the following code block:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# robot clearning with single goal\n",
    "state = room_3.reset()\n",
    "for _ in range(100):\n",
    "    action = robot_3.decide(state)\n",
    "    state, reward, done = room_3.step(action)\n",
    "    room_3.render()\n",
    "    plt.pause(0.01)\n",
    "    display.clear_output(wait=True)\n",
    "    if done:\n",
    "        state = room_3.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the three learning scenarios, you should notice the following:\n",
    "* The epsilon-greedy algorithm balances between exploration and exploitation by using the probabilities ε and 1 − ε respectively.\n",
    "* A simple environment (as in [Section 5.1](#section_51)) with no obstacle and only one cell to clean requires fewer than 10000 learning episode to achieve convergence.\n",
    "* As the environment becomes more complex (as in [Section 5.2](#section_52) and [Section 5.3](#section_53)), the number of episodes required to find the maximum rewards increases exponentially."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Marking Scheme and Submission <a id='section_6'></a>\n",
    "\n",
    "This part carries 30% of the assignment grade. Part A (clustering) carries 50%. The Quiz posted on Moodle carries 20%. Late submission will incur a 30% deduction. The marking scheme of this part follows.\n",
    "\n",
    "**Task Summary**\n",
    "\n",
    "| Task | Grade Points \n",
    "|:----|:----:         \n",
    "| 1. Parameter Initialization ([`QAgent.__init__()`](#task_1)) | 3\n",
    "| 2. Action Sampling ([`QAgent.decide()`](#task_2)) | 4\n",
    "| 3. Q-Learning Function ([`QAgent.learn()`](#task_3)) | 6\n",
    "| 4. Agent-Environment Interaction ([`agent_env_interaction()`](#task_4)) | 5\n",
    "| 5. Learning Process ([`agent_env_interaction()`](#task_5)) | 3\n",
    "| 6. [Empty Room with One Cell to Clean](#task_6) | 3\n",
    "| 7. [Empty Room with Two Cells to Clean](#task_7) | 3\n",
    "| 8. [Room with Obstacles and Two Cells to Clean](#task_8) | 3\n",
    "| **TOTAL** | **30**\n",
    "\n",
    "### Submission\n",
    "You are required to upload to Moodle a zip file containing the following files.\n",
    "\n",
    "1. Your completed Jupyter Notebook of this part. Please rename your file as `A3B_[SID]_[FirstnameLastname].ipynb` (where `[SID]` is your student ID and `[FirstnameLastname]` is your first name and last name concatenated) and do not include the data file. You must complete the **Acknowledgment** section in order for the file to be graded.\n",
    "2. The PDF version (.pdf file) of your completed notebook (click `File > Download as > PDF via HTML` (If error occurs, you may download it as HTML and then save the HTML as PDF separately)).\n",
    "\n",
    "In addition, please complete **A3Q: Assignment 3 -- Quiz** separately on the Moodle site."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Summary <a id='section_7'></a>\n",
    "Congratulations! You have implemented the Q-learning algorithm to enable a robot to clean a room using reinforcement learning! To summarize, you have implemented the Q-agent and the interaction process between agent and environment. Your program has evaluated the agent's learning in three different environments and has produced visualizations of the reward and robot behavior. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c3bb2cce89c6c7f35f0b91ef8aafcf4ad10dc9f6a12587352a7ca972e5c0cb5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
